{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-07T11:28:57.985684Z","iopub.status.idle":"2024-06-07T11:29:00.123123Z","shell.execute_reply.started":"2024-06-07T11:28:57.986190Z","shell.execute_reply":"2024-06-07T11:29:00.121997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\n\n\nclass BirdDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label_str = self._get_audio_sample_label(index)  # Récupérer la valeur retournée par _get_audio_sample_label(index)\n    \n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        \n        return signal, [label_str]  # Retourner une liste contenant le label sous forme de chaîne de caractères\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 11])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return [str(self.annotations.iloc[index, 0])]  # Retourner le label sous forme de liste contenant une chaîne de caractères\n\n\nif __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/birdclef-2024/train_metadata.csv\"\n    AUDIO_DIR = \"/kaggle/input/birdclef-2024/train_audio\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = BirdDataset(ANNOTATIONS_FILE,\n                      AUDIO_DIR,\n                      mel_spectrogram,\n                      SAMPLE_RATE,\n                      NUM_SAMPLES,\n                      device)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    print(usd[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:29:00.125244Z","iopub.execute_input":"2024-06-07T11:29:00.125798Z","iopub.status.idle":"2024-06-07T11:29:00.300686Z","shell.execute_reply.started":"2024-06-07T11:29:00.125757Z","shell.execute_reply":"2024-06-07T11:29:00.299640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:29:00.302146Z","iopub.execute_input":"2024-06-07T11:29:00.302486Z","iopub.status.idle":"2024-06-07T11:29:12.789514Z","shell.execute_reply.started":"2024-06-07T11:29:00.302453Z","shell.execute_reply":"2024-06-07T11:29:12.788436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cnn.py\nfrom torch import nn\nfrom torchsummary import summary\n\nclass CNNNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Quatre couches convolutionnelles\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128*5*4, 183)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions\n\nif __name__ == \"__main__\":\n    cnn = CNNNetwork()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    cnn.to(device)  # Transférer le modèle sur le GPU\n    summary(cnn, (1, 64, 44))","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:29:12.792664Z","iopub.execute_input":"2024-06-07T11:29:12.793087Z","iopub.status.idle":"2024-06-07T11:29:12.821025Z","shell.execute_reply.started":"2024-06-07T11:29:12.793048Z","shell.execute_reply":"2024-06-07T11:29:12.820062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nANNOTATIONS_FILE = \"/kaggle/input/birdclef-2024/train_metadata.csv\"\nAUDIO_DIR = \"/kaggle/input/birdclef-2024/train_audio\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050\n\nBATCH_SIZE = 128\nEPOCHS = 2\nLEARNING_RATE = 0.001\n\nclass BirdDataset(torch.utils.data.Dataset):\n    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n        self.resampler = torchaudio.transforms.Resample(orig_freq=target_sample_rate, new_freq=target_sample_rate).to(self.device)\n        self.label_encoder = LabelEncoder()\n        \n        # Avant d'instancier l'ensemble de données, ajustez l'encodeur de libellé sur l'ensemble des étiquettes de l'ensemble de données d'entraînement\n        train_labels = self.annotations.iloc[:, 0].tolist()\n        self.label_encoder.fit(train_labels)\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label_str = self._get_audio_sample_label(index)\n        \n        label = encode_labels(self.label_encoder, [label_str])  # Utiliser l'encodeur ajusté pour transformer les étiquettes\n        \n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 11])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 0] \n\n\ndef encode_labels(label_encoder, labels):\n    encoded_labels = label_encoder.transform(labels)\n    return torch.tensor(encoded_labels, dtype=torch.long)\n\n\ndef create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    epoch_loss = 0\n    all_predictions = []\n    all_targets = []\n    for input, target_str in data_loader:\n        input = input.to(device)\n        target = target_str.to(device)\n\n        prediction = model(input)\n        loss = loss_fn(prediction, target.squeeze())\n        epoch_loss += loss.item()\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n        all_predictions.extend(torch.argmax(prediction, dim=1).cpu().numpy())\n        all_targets.extend(target.squeeze().cpu().numpy())\n\n    accuracy = accuracy_score(all_targets, all_predictions)\n    print(f'Accuracy: {accuracy:.4f}')\n    return epoch_loss / len(data_loader), accuracy\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    losses = []\n    accuracies = []\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        epoch_loss, accuracy = train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        losses.append(epoch_loss)\n        accuracies.append(accuracy)\n        print(f\"Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n        print(\"---------------------------\")\n    print(\"Finished training\")\n\n    plt.plot(range(epochs), losses, marker='o', label='Loss')\n    plt.plot(range(epochs), accuracies, marker='o', label='Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Value')\n    plt.title('Loss and Accuracy vs Epochs')\n    plt.legend()\n    plt.show()\n\n\nclass CNNNetwork(nn.Module):\n    def __init__(self):\n        super(CNNNetwork, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(2560, 183)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        x = self.linear(x)\n        x = self.softmax(x)\n        return x\n\n\nif __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = BirdDataset(ANNOTATIONS_FILE,\n                      AUDIO_DIR,\n                      mel_spectrogram,\n                      SAMPLE_RATE,\n                      NUM_SAMPLES,\n                      device)\n    \n    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(cnn.parameters(),\n                                 lr=LEARNING_RATE)\n\n    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n    print(\"Trained feed forward net saved at feedforwardnet.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:29:48.221694Z","iopub.execute_input":"2024-06-07T11:29:48.222111Z","iopub.status.idle":"2024-06-07T11:30:06.986879Z","shell.execute_reply.started":"2024-06-07T11:29:48.222077Z","shell.execute_reply":"2024-06-07T11:30:06.985203Z"},"trusted":true},"execution_count":null,"outputs":[]}]}